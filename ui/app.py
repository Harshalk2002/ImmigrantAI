# -*- coding: utf-8 -*-
"""Agentic_System.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aGHzlaDDplncWJZRLL8zuWyk0VlNtXkf
"""

# # Agent 1 â€” RAG Immigration Policy Summarizer

# This notebook builds **Agent 1** for our project:

# - Connects to the existing **ChromaDB** with USCIS / DHS chunks
# - Uses **LangChain + OpenAI** to summarize immigration rules
# - Extracts **CPT / OPT rules for F-1 students** into a structured JSON file
# - Saves the result as `agent1_output.json` for Agent 2 to consume

# SECTION 1 â€” Install dependencies (Colab only)
!pip install -q langchain langchain-openai langchain-community chromadb python-dotenv

# ## Section 2 â€” Configure API key and mount Google Drive

# - Sets the `OPENAI_API_KEY` (do **NOT** hard-code it in the notebook you share)
# - Mounts Google Drive so we can access the existing Chroma DB at
#   `/content/drive/MyDrive/chroma_db (1)`
import os
from google.colab import drive

# ğŸ‘‰ SAFER: type your key when running; do NOT commit it to Git / share it
from getpass import getpass
os.environ["OPENAI_API_KEY"] = getpass("Enter your key or use .env file ")

# Mount Google Drive (contains Jared's Chroma DB)
drive.mount("/content/drive")

# List the Chroma DB directory to confirm it's available
!ls "/content/drive/MyDrive/chroma_db (1)"

# ## Section 3 â€” Define Agent 1 RAG class

# This section:

# - Defines `Agent1Config` with:
#   - `persist_dir` â†’ existing Chroma DB path
#   - `collection_name` â†’ `"uscis_f1_h1b_employment"`
#   - `embedding_model` â†’ same HF model used to build the DB
# - Defines `Agent1RAG` which:
#   - Connects to Chroma
#   - Retrieves top-k chunks
#   - Formats them into a context block
#   - Calls `gpt-4o-mini` to summarize policies

from __future__ import annotations
from typing import List, Dict, Any, Optional
from dataclasses import dataclass

from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_core.prompts import ChatPromptTemplate

load_dotenv()


@dataclass
class Agent1Config:
    # EXACT location of Chroma DB
    persist_dir: str = "/content/drive/MyDrive/chroma_db (1)"
    # REAL collection with USCIS + travel.gov chunks
    collection_name: str = "uscis_f1_h1b_employment"

    # LLM used for summarization
    model_name: str = "gpt-4o-mini"

    # SAME embedding model used to build the DB
    embedding_model: str = "sentence-transformers/all-MiniLM-L6-v2"

    k: int = 6
    # No score filter for debugging; can raise later (e.g., 0.2â€“0.3)
    min_relevance_threshold: float = 0.0


class Agent1RAG:
    def __init__(self, config: Optional[Agent1Config] = None):
        self.config = config or Agent1Config()

        # Use SAME embedding model as index
        self.embeddings = HuggingFaceEmbeddings(
            model_name=self.config.embedding_model
        )

        # Connect to existing Chroma DB
        self.vstore = Chroma(
            collection_name=self.config.collection_name,
            embedding_function=self.embeddings,
            persist_directory=self.config.persist_dir,
        )

        # LLM
        self.llm = ChatOpenAI(model=self.config.model_name, temperature=0.1)

        # Policy-summarizer prompt
        self.prompt = ChatPromptTemplate.from_template(
            """
You are Agent 1, an immigration policy summarizer.

Your job:
- Read retrieved chunks.
- Summarize the main rules.
- Extract concrete action items for the student.
- Rate risk as LOW / MEDIUM / HIGH.
- Use ONLY the retrieved text; if context is weak, say so.

User question:
{question}

Retrieved context:
{context}
"""
        )

        # DEBUG: show how many docs are in the collection
        try:
            print("DEBUG: documents in collection:", self.vstore._collection.count())
        except Exception as e:
            print("DEBUG: could not read collection count:", e)

    def retrieve(self, question: str, filters: dict | None = None, k: int | None = None):
        k = k or self.config.k
        results = self.vstore.similarity_search_with_relevance_scores(
            question,
            k=k,
            filter=filters,
        )

        out = []
        for doc, score in results:
            out.append(
                {
                    "text": doc.page_content,
                    "metadata": doc.metadata,
                    "score": score,
                }
            )

        print("DEBUG: retrieved", len(out), "docs")
        return out

    def _format_context(self, retrieved):
        if not retrieved:
            return "NO RELEVANT SNIPPETS FOUND."

        blocks = []
        for i, r in enumerate(retrieved, 1):
            m = r["metadata"]
            blocks.append(
                f"[Snippet {i}]\n"
                f"Score: {r['score']:.3f}\n"
                f"Source: {m.get('source_type', m.get('source_name', 'unknown'))}\n"
                f"Metadata: {m}\n\n"
                f"{r['text']}"
            )
        return "\n\n".join(blocks)

    def answer_question(self, question: str, filters=None, k=None) -> str:
        retrieved = self.retrieve(question, filters, k)
        context = self._format_context(retrieved)
        messages = self.prompt.format_messages(
            question=question,
            context=context,
        )
        response = self.llm.invoke(messages)
        return response.content


# IMPORTANT: create the agent
agent1 = Agent1RAG()

# ## Section 3 â€” Define Agent 1 RAG class

# This section:
# - Defines `Agent1Config` with:
#   - `persist_dir` â†’ existing Chroma DB path
#   - `collection_name` â†’ `"uscis_f1_h1b_employment"`
#   - `embedding_model` â†’ same HF model used to build the DB
# - Defines `Agent1RAG` which:
#   - Connects to Chroma
#   - Retrieves top-k chunks
#   - Formats them into a context block
#   - Calls `gpt-4o-mini` to answer question-specific policy queries

from __future__ import annotations
from typing import List, Dict, Any, Optional
from dataclasses import dataclass

from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_core.prompts import ChatPromptTemplate

load_dotenv()


@dataclass
class Agent1Config:
    # EXACT location of Chroma DB
    persist_dir: str = "/content/drive/MyDrive/chroma_db (1)"
    # REAL collection with USCIS + travel.gov chunks
    collection_name: str = "uscis_f1_h1b_employment"

    # LLM used for answering
    model_name: str = "gpt-4o-mini"

    # SAME embedding model used to build the DB
    embedding_model: str = "sentence-transformers/all-MiniLM-L6-v2"

    k: int = 6
    # No score filter for debugging; can raise later (e.g., 0.2â€“0.3)
    min_relevance_threshold: float = 0.0


class Agent1RAG:
    def __init__(self, config: Optional[Agent1Config] = None):
        self.config = config or Agent1Config()

        # Use SAME embedding model as index
        self.embeddings = HuggingFaceEmbeddings(
            model_name=self.config.embedding_model
        )

        # Connect to existing Chroma DB
        self.vstore = Chroma(
            collection_name=self.config.collection_name,
            embedding_function=self.embeddings,
            persist_directory=self.config.persist_dir,
        )

        # LLM
        self.llm = ChatOpenAI(model=self.config.model_name, temperature=0.1)

        # â— Question-aware policy prompt (not just "summarize everything")
        self.prompt = ChatPromptTemplate.from_template(
            """
You are Agent 1, an expert assistant on U.S. F-1, CPT, OPT, STEM OPT, and H-1B rules.

Your job:
- Read the retrieved policy context.
- Answer the user's **specific question** as clearly as possible.
- Use ONLY the retrieved text; if the answer is not clearly supported, say you are not sure
  and recommend checking with the DSO or USCIS.
- Use bullet points when listing conditions, steps, or requirements.
- Be concise but precise. Do NOT invent rules or dates that are not in the context.

User question:
{question}

Retrieved context:
{context}
"""
        )

        # DEBUG: show how many docs are in the collection
        try:
            print("DEBUG: documents in collection:", self.vstore._collection.count())
        except Exception as e:
            print("DEBUG: could not read collection count:", e)

    def retrieve(
        self,
        question: str,
        filters: dict | None = None,
        k: int | None = None,
    ) -> List[Dict[str, Any]]:
        """
        Retrieve top-k relevant chunks from Chroma and return a list of dicts:
        [{ "text": ..., "metadata": {...}, "score": ... }, ...]
        """
        k = k or self.config.k
        results = self.vstore.similarity_search_with_relevance_scores(
            question,
            k=k,
            filter=filters,
        )

        out: List[Dict[str, Any]] = []
        for doc, score in results:
            out.append(
                {
                    "text": doc.page_content,
                    "metadata": doc.metadata,
                    "score": score,
                }
            )

        print("DEBUG: retrieved", len(out), "docs")
        return out

    def _format_context(self, retrieved: List[Dict[str, Any]]) -> str:
        """
        Turn retrieved chunks into a readable context block for the prompt.
        """
        if not retrieved:
            return "NO RELEVANT SNIPPETS FOUND."

        blocks = []
        for i, r in enumerate(retrieved, 1):
            m = r["metadata"]
            blocks.append(
                f"[Snippet {i}]\n"
                f"Score: {r['score']:.3f}\n"
                f"Source: {m.get('source_type', m.get('source_name', 'unknown'))}\n"
                f"Metadata: {m}\n\n"
                f"{r['text']}"
            )
        return "\n\n".join(blocks)

    def answer_question(self, question: str, filters=None, k=None) -> str:
        """
        Legacy helper: keep this if you still want a simple one-shot Q&A.
        Agent-2 uses answer_from_rag() below instead.
        """
        retrieved = self.retrieve(question, filters, k)
        context = self._format_context(retrieved)
        messages = self.prompt.format_messages(
            question=question,
            context=context,
        )
        response = self.llm.invoke(messages)
        return response.content


# IMPORTANT: create the global Agent-1 instance
agent1 = Agent1RAG()


def answer_from_rag(user_question: str, chat_history: list | None = None) -> dict:
    """
    Wrapper used by Agent 2 (agents_backend.py).

    Inputs:
        - user_question: the latest user message
        - chat_history: full chat (currently unused, but kept for future extensions)

    Returns:
        {
          "answer": <LLM markdown string>,
          "sources": [ { "text": ..., "metadata": {...}, "score": ... }, ... ]
        }
    """

    # 1. Retrieve relevant chunks from Chroma
    retrieved = agent1.retrieve(user_question)

    # 2. Format context for the prompt
    context = agent1._format_context(retrieved)

    # 3. Build messages and call the LLM
    messages = agent1.prompt.format_messages(
        question=user_question,
        context=context,
    )
    response = agent1.llm.invoke(messages)

    # 4. Return answer + retrieved docs as sources
    return {
        "answer": response.content,
        "sources": retrieved,
    }

from agent1_rag import answer_from_rag

questions = [
    "What are the H-1B requirements?",
    "When is the earliest I can file my H-1B petition?",
    "What is a specialty occupation in the H-1B context?",
    "Can full-time CPT affect my eligibility for OPT?",
]

for q in questions:
    print("\n==============================")
    print("QUESTION:", q)
    out = answer_from_rag(q, [])
    print("ANSWER:\n", out["answer"][:800])  # print first ~800 chars

### EVALUATION
from agent1_rag import answer_from_rag

# 1. Define test questions and the key phrases we *expect* to see
eval_items = [
    {
        "q": "What are the H-1B requirements?",
        "must_have": ["specialty occupation", "bachelor", "October 1"]
    },
    {
        "q": "When is the earliest I can file my H-1B petition?",
        "must_have": ["6 months", "October 1"]
    },
    {
        "q": "What is a specialty occupation in the H-1B context?",
        "must_have": ["highly specialized", "bachelorâ€™s degree"]
    },
    {
        "q": "Can full-time CPT affect my eligibility for OPT?",
        "must_have": ["ineligible", "post-completion OPT"]
    },
]

results = []
for item in eval_items:
    q = item["q"]
    expected = item["must_have"]
    ans = answer_from_rag(q, [])

    # Fix: Access the 'answer' key from the dictionary returned by answer_from_rag
    hits = [kw for kw in expected if kw.lower() in ans["answer"].lower()]
    recall = len(hits) / len(expected)

    results.append({
        "question": q,
        "answer_preview": ans["answer"][:250] + "...", # Also update answer_preview
        "expected_keywords": expected,
        "hits": hits,
        "recall": round(recall, 2),
    })

for r in results:
    print("QUESTION:", r["question"])
    print("Recall:", r["recall"], "Hits:", r["hits"])
    print("Preview:", r["answer_preview"])
    print("-" * 80)

# ## Section 4 â€” Quick debug question (optional)

# Just to sanity-check that Agent 1 can talk to the DB + model.
# You can keep or delete this from the final version.
test_response = agent1.answer_question("Briefly summarize H-1B visa requirements.")
print(test_response[:1000])  # print first 1000 chars

### section 5
import json

prompt_for_rules = """
Return ONLY a valid JSON list.

Each item must contain:
- update
- source
- risk_level
- action_needed

Summarize all CPT and OPT rules for F-1 students.
"""

real_output = agent1.answer_question(prompt_for_rules)
print("RAW MODEL OUTPUT (first 1000 chars):\n")
print(real_output[:1000])

# Pre-process the output to remove markdown code block fences if present
if real_output.strip().startswith('```json') and real_output.strip().endswith('```'):
    real_output = real_output.strip()[7:-3].strip()

# Parse and validate JSON
try:
    agent1_updates = json.loads(real_output)
    assert isinstance(agent1_updates, list)
except Exception as e:
    print("\nâŒ ERROR: Model did not return a valid JSON list.")
    print("Exception:", e)
    raise

# Save clean JSON for Agent 2
with open("agent1_output.json", "w") as f:
    json.dump(agent1_updates, f, indent=4)

print("\nâœ… Saved agent1_output.json")
print("Total rules:", len(agent1_updates))

## Section 6 â€” Download JSON for sharing / debugging

# Use this in Colab to download `agent1_output.json` locally,
# or just keep it in the notebook directory so Agent 2 can load it
# with `load_agent1_output("agent1_output.json")`.
from google.colab import files

files.download("agent1_output.json")

# ## Section 6 â€” Agent 2 Backend (`agents_backend.py`) â€” Pavithra

# This module:

# - Loads **Agent 1** JSON (`agent1_output.json`)
# - Computes CPT / OPT timeline
# - Generates a structured checklist
# - Provides `chat_reply()` for chat UI
# - This is the production backend used by Streamlit.

# Note: `load_agent1_output` is robust and supports both:
# - A plain JSON list `[ {...}, {...} ]`
# - Or a dict `{"agent1_summary": [ ... ]}` (older format)

# Commented out IPython magic to ensure Python compatibility.
# %%writefile agents_backend.py
# import json
# import datetime
# from dateutil.relativedelta import relativedelta
# from typing import List, Dict, Any, Optional
# 
# from agent1_rag import answer_from_rag  # ğŸ”¥ live RAG
# 
# 
# # ============================================================
# # 0. LOAD AGENT 1 OUTPUT (JSON) â€“ still used for checklist
# # ============================================================
# 
# def load_agent1_output(file_path: str = "agent1_output.json") -> list:
#     try:
#         with open(file_path, "r") as f:
#             data = json.load(f)
#     except Exception:
#         return []
# 
#     if isinstance(data, dict) and "agent1_summary" in data:
#         return data["agent1_summary"]
# 
#     if isinstance(data, list):
#         return data
# 
#     return []
# 
# 
# # ============================================================
# # 1. DATE UTILITIES
# # ============================================================
# 
# def parse_date(date_str: str) -> datetime.date:
#     return datetime.datetime.strptime(date_str, "%Y-%m-%d").date()
# 
# 
# def months_between(d1: datetime.date, d2: datetime.date) -> int:
#     return (d2.year - d1.year) * 12 + (d2.month - d1.month)
# 
# 
# # ============================================================
# # 2. TIMELINE ENGINE
# # ============================================================
# 
# def compute_timeline(user_profile: dict) -> dict:
#     program_start = parse_date(user_profile["program_start"])
#     arrival = program_start
#     grad = program_start + relativedelta(years=2)
#     today = datetime.date.today()
# 
#     return {
#         "arrival_date": arrival.isoformat(),
#         "graduation_date": grad.isoformat(),
#         "cpt_eligibility_date": (arrival + relativedelta(months=12)).isoformat(),
#         "opt_eligibility_start": (grad - relativedelta(days=90)).isoformat(),
#         "opt_eligibility_end": grad.isoformat(),
#         "months_since_arrival": months_between(arrival, today),
#         "today": today.isoformat(),
#     }
# 
# 
# # ============================================================
# # 3. CHECKLIST GENERATOR
# # ============================================================
# 
# def generate_checklist(full_profile: dict, timeline: dict, agent1_updates: list) -> list:
#     checklist = []
# 
#     # Month 1 Tasks
#     checklist.append({
#         "task": "Complete SEVIS Check-In",
#         "due_date": timeline["arrival_date"],
#         "category": "SEVIS Compliance",
#         "reason": "Required in first 30 days of arrival",
#     })
# 
#     checklist.append({
#         "task": "Upload Passport to ISSS Portal",
#         "due_date": timeline["arrival_date"],
#         "category": "Documentation",
#         "reason": "DSO must verify identity",
#     })
# 
#     # Month 6 Tasks
#     checklist.append({
#         "task": "Meet Academic Advisor",
#         "due_date": "N/A",
#         "category": "Academic",
#         "reason": "Check progress and confirm full-time enrollment",
#     })
# 
#     # CPT Eligibility
#     checklist.append({
#         "task": "CPT Eligibility Begins",
#         "due_date": timeline["cpt_eligibility_date"],
#         "category": "CPT",
#         "reason": "12 months of full-time enrollment required before CPT",
#     })
# 
#     checklist.append({
#         "task": "Request CPT I-20 Endorsement",
#         "due_date": timeline["cpt_eligibility_date"],
#         "category": "CPT",
#         "reason": "DSO must issue new I-20 with CPT authorization",
#     })
# 
#     # OPT Window
#     checklist.append({
#         "task": "Prepare OPT Application",
#         "due_date": timeline["opt_eligibility_start"],
#         "category": "OPT",
#         "reason": "You can apply for OPT 90 days before graduation",
#     })
# 
#     # Policy-triggered tasks from Agent 1 JSON
#     for rule in agent1_updates:
#         checklist.append({
#             "task": rule.get("action_needed", "Follow policy update"),
#             "due_date": "N/A",
#             "category": "Policy Update",
#             "reason": rule.get("update", ""),
#         })
# 
#     return checklist
# 
# 
# # ============================================================
# # 4. MAIN TIMELINE + CHECKLIST COORDINATOR
# # ============================================================
# 
# def generate_timeline_and_checklist(
#     user_profile: dict,
#     agent1_output: list | None = None
# ) -> dict:
#     if agent1_output is None:
#         agent1_output = load_agent1_output()
# 
#     program_start = user_profile.get("program_start")
#     if not program_start:
#         raise ValueError("program_start is required in user_profile")
# 
#     arrival_date = program_start
#     grad_date = (parse_date(program_start) + relativedelta(years=2)).isoformat()
# 
#     full_profile = {
#         "name": user_profile.get("name"),
#         "country": user_profile.get("country"),
#         "dob": user_profile.get("dob"),
#         "arrival_date": arrival_date,
#         "major": "Unknown",
#         "degree_level": "Masters",
#         "graduation_date": grad_date,
#         "milestones": {
#             "sevis_checkin": False,
#             "passport_uploaded": False,
#             "first_semester_complete": False,
#             "cpt_applied": False,
#             "opt_applied": False,
#         },
#     }
# 
#     timeline = compute_timeline({"program_start": program_start})
#     checklist = generate_checklist(full_profile, timeline, agent1_output)
# 
#     return {
#         "user_profile": full_profile,
#         "timeline": timeline,
#         "checklist": checklist,
#         "agent1_updates_used": agent1_output,
#     }
# 
# 
# # ============================================================
# # 5. POLICY QUESTION ROUTING (for RAG)
# # ============================================================
# 
# POLICY_KEYWORDS = [
#     "h-1b", "h1b", "h 1b",
#     "opt", "stem opt", "pre-completion opt", "post-completion opt",
#     "cpt", "curricular practical training",
#     "f-1", "f1",
#     "visa", "uscis", "sevis",
#     "i-20", "i 20", "i20",
#     "status", "grace period", "cap gap",
# ]
# 
# def is_policy_question(text: str) -> bool:
#     t = text.lower()
#     return any(kw in t for kw in POLICY_KEYWORDS)
# 
# 
# def _format_timeline_block(tl: dict) -> str:
#     return (
#         "### Timeline Summary\n"
#         f"- Arrival / Program Start: **{tl['arrival_date']}**\n"
#         f"- CPT Eligibility Date: **{tl['cpt_eligibility_date']}**\n"
#         f"- OPT Application Window: **{tl['opt_eligibility_start']} â†’ {tl['opt_eligibility_end']}**\n"
#         f"- Assumed Graduation Date: **{tl['graduation_date']}**\n"
#         f"- Months Since Arrival (approx): **{tl['months_since_arrival']}**\n"
#     )
# 
# 
# def _format_sources(sources: List[Dict[str, Any]]) -> str:
#     if not sources:
#         return ""
#     lines = ["\n\n---\n**Sources (from USCIS / official docs)**\n"]
#     for i, s in enumerate(sources, start=1):
#         m = s.get("metadata", {})
#         label = m.get("source_type") or m.get("source_name") or f"chunk_{i}"
#         lines.append(f"- {label}")
#     return "\n".join(lines)
# 
# 
# # ============================================================
# # 6. CHATBOT ENTRY POINT (Agent 2 orchestrator)
# # ============================================================
# 
# def chat_reply(user_profile: dict, chat_history: list) -> str:
#     """
#     Main function called by chat UI.
# 
#     - For H-1B / CPT / OPT / visa / USCIS questions:
#         â†’ Calls Agent-1 RAG (answer_from_rag)
#         â†’ Attaches sources + timeline.
#     - For "timeline/checklist" questions:
#         â†’ Only timeline summary is returned.
#     """
# 
#     if not chat_history:
#         return "Hi! Iâ€™m your F-1 Visa assistant. Ask me anything about CPT, OPT, H-1B, or your timeline."
# 
#     # Get last user message
#     last_user_msg = ""
#     for msg in reversed(chat_history):
#         if msg.get("role") == "user":
#             last_user_msg = msg.get("content", "")
#             break
# 
#     if not last_user_msg.strip():
#         return "Hi! Ask me anything about CPT, OPT, H-1B, or your F-1 status."
# 
#     # Lowercase + normalize unicode dashes for reliable keyword matching (H-1B etc.)
#     last_lower = last_user_msg.lower()
#     last_lower = (
#         last_lower.replace("â€“", "-")   # en dash
#                   .replace("â€”", "-")   # em dash
#                   .replace("âˆ’", "-")   # minus sign
#     )
# 
#     # Run Agent 2 pipeline (timeline + checklist)
#     agent1_output = load_agent1_output()
#     pipeline = generate_timeline_and_checklist(user_profile, agent1_output)
#     tl = pipeline["timeline"]
# 
#     parts: List[str] = []
# 
#     if is_policy_question(last_lower):
#         # ğŸ”¥ THIS IS WHERE WE HIT AGENT-1 + CHROMA
#         rag_result = answer_from_rag(last_user_msg, chat_history)
#         parts.append("## Policy Answer (Agent-1 RAG)\n")
#         parts.append(rag_result["answer"])
#         parts.append(_format_sources(rag_result["sources"]))
#     elif "timeline" in last_lower or "checklist" in last_lower:
#         parts.append(
#             "Hereâ€™s a summary of your F-1 / CPT / OPT timeline based on your program start date.\n"
#         )
#     else:
#         parts.append(
#             "Iâ€™m not sure if thatâ€™s a specific immigration policy question, "
#             "but here is your current CPT / OPT timeline summary."
#         )
# 
#     parts.append("\n" + _format_timeline_block(tl))
#     return "\n\n".join(parts)
# 
#

# Commented out IPython magic to ensure Python compatibility.
# # ## Section 7 â€” Streamlit Chat UI (`app.py`)
# 
# # Simple chat interface that:
# 
# # - Collects a minimal `user_profile`
# # - Keeps chat history in `st.session_state`
# # - Calls `chat_reply(user_profile, chat_history)` from `agents_backend.py`
# %%writefile app.py
# import streamlit as st
# import datetime
# 
# from agents_backend import chat_reply, generate_timeline_and_checklist, load_agent1_output
# 
# st.set_page_config(page_title="F-1 Visa Assistant", page_icon="ğŸ“", layout="wide")
# 
# st.title("ğŸ“ F-1 Visa Assistant (Agent 1 + Agent 2)")
# 
# st.markdown(
#     "This app uses **Agent 1 (RAG)** over USCIS rules and **Agent 2** "
#     "to compute your CPT / OPT timeline and checklist."
# )
# 
# # ---------------------------------------------
# # Sidebar â€“ user profile
# # ---------------------------------------------
# st.sidebar.header("Student Profile")
# 
# name = st.sidebar.text_input("Full Name", "Test Student")
# country = st.sidebar.text_input("Country of Citizenship", "India")
# 
# dob = st.sidebar.date_input(
#     "Date of Birth",
#     value=datetime.date(2000, 1, 1),
#     min_value=datetime.date(1900, 1, 1),
#     max_value=datetime.date(2100, 12, 31),
# )
# 
# program_start = st.sidebar.date_input(
#     "Program Start Date",
#     value=datetime.date(2024, 8, 15),
#     min_value=datetime.date(2000, 1, 1),
#     max_value=datetime.date(2100, 12, 31),
# )
# 
# user_profile = {
#     "name": name,
#     "country": country,
#     "dob": dob.isoformat(),
#     "program_start": program_start.isoformat(),
# }
# 
# # Pre-compute timeline + checklist so sidebar data is used everywhere
# agent1_updates = load_agent1_output()
# timeline_output = generate_timeline_and_checklist(user_profile, agent1_updates)
# 
# # ---------------------------------------------
# # Main chat area
# # ---------------------------------------------
# st.subheader("ğŸ’¬ Chat")
# 
# if "chat_history" not in st.session_state:
#     st.session_state.chat_history = []
# 
# for msg in st.session_state.chat_history:
#     role = "user" if msg["role"] == "user" else "assistant"
#     with st.chat_message(role):
#         st.markdown(msg["content"])
# 
# user_msg = st.chat_input("Ask about CPT, OPT, or your timeline...")
# if user_msg:
#     st.session_state.chat_history.append({"role": "user", "content": user_msg})
# 
#     reply = chat_reply(user_profile, st.session_state.chat_history)
#     st.session_state.chat_history.append({"role": "assistant", "content": reply})
# 
#     with st.chat_message("assistant"):
#         st.markdown(reply)
# 
# # ---------------------------------------------
# # Optional: show raw timeline & checklist
# # ---------------------------------------------
# with st.expander("ğŸ“… Raw Timeline & Checklist (debug)"):
#     st.json(timeline_output)
# 
#

# ## Section 8 â€” Streamlit Checklist Page (`pages/Checklist.py`)

# This page is basically Pavithraâ€™s **Checklist / Timeline dashboard**:

# - Loads `agent1_output.json`
# - Runs `generate_timeline_and_checklist`
# - Displays timeline + detailed checklist
# - Shows which Agent 1 rules were used

# import os

# # Create the 'pages' directory if it doesn't exist
# if not os.path.exists('pages'):
#     os.makedirs('pages')

# Commented out IPython magic to ensure Python compatibility.
# %%writefile pages/Checklist.py
# import streamlit as st
# import datetime
# import json
# 
# from agents_backend import (
#     generate_timeline_and_checklist,
#     load_agent1_output,
# )
# 
# st.set_page_config(page_title="F-1 Checklist", page_icon="ğŸ“‹", layout="wide")
# 
# st.title("ğŸ“‹ F-1 Visa Checklist & Timeline")
# 
# st.write(
#     """
# This page validates **Agent 1 â†’ Agent 2 integration** and displays the resulting CPT/OPT
# timeline and checklist. It also prints debugging information so the team can confirm
# that Agent 1 JSON rules are being used.
# """
# )
# 
# # ---------------------------------------------
# # USER INPUT FORM
# # ---------------------------------------------
# st.subheader("ğŸ”¹ Student Profile")
# 
# with st.form("profile_form"):
#     col1, col2 = st.columns(2)
# 
#     with col1:
#         name = st.text_input("Full Name", "")
#         country = st.text_input("Country of Citizenship", "")
# 
#     with col2:
#         dob = st.date_input(
#             "Date of Birth",
#             value=datetime.date(2000, 1, 1),
#             min_value=datetime.date(1900, 1, 1),
#             max_value=datetime.date(2100, 12, 31),
#         )
#         program_start = st.date_input(
#             "Program Start Date",
#             value=datetime.date(2024, 8, 15),
#             min_value=datetime.date(2000, 1, 1),
#             max_value=datetime.date(2100, 12, 31),
#         )
# 
#     submitted = st.form_submit_button("Generate Checklist")
# 
# # ---------------------------------------------
# # PROCESS FORM SUBMISSION
# # ---------------------------------------------
# if submitted:
#     profile = {
#         "name": name,
#         "country": country,
#         "dob": dob.isoformat(),
#         "program_start": program_start.isoformat(),
#     }
# 
#     agent1_updates = load_agent1_output()
# 
#     # Run Agent 2
#     output = generate_timeline_and_checklist(profile, agent1_updates)
#     timeline = output["timeline"]
#     checklist = output["checklist"]
# 
#     st.success("Checklist generated using Agent 1 + Agent 2 logic.")
# 
#     # TIMELINE
#     st.subheader("ğŸ“… Visa Timeline")
#     st.markdown(
#         f"""
# **Arrival / Program Start:** {timeline['arrival_date']}
# **CPT Eligibility:** {timeline['cpt_eligibility_date']}
# **OPT Application Window:** {timeline['opt_eligibility_start']} â†’ {timeline['opt_eligibility_end']}
# **Months Since Arrival:** {timeline['months_since_arrival']}
# """
#     )
# 
#     # CHECKLIST
#     st.subheader("ğŸ—‚ Detailed Checklist")
#     for item in checklist:
#         st.markdown(f"### {item['task']}")
#         st.write(f"- **Due Date:** {item['due_date']}")
#         st.write(f"- **Category:** {item['category']}")
#         st.write(f"- **Reason:** {item['reason']}")
#         st.markdown("---")
# 
#     # POLICY RULES USED
#     st.subheader("ğŸ“˜ Agent 1 Policy Rules Used")
#     for rule in agent1_updates:
#         st.markdown(
#             f"""
# **Rule:** {rule.get('update', '')}
# **Source:** {rule.get('source', '')}
# **Action Required:** {rule.get('action_needed', '')}
# **Risk Level:** `{rule.get('risk_level', '')}`
# ---
# """
#         )
#

# ## Section 9 â€” Direct terminal pipeline validation

# Runs Agent 2 directly (no Streamlit) using:

# - `agent1_output.json`
# - A test profile

# Prints the full combined output (timeline + checklist).
import json
from agents_backend import load_agent1_output, generate_timeline_and_checklist

print("\n\n===== STARTING AGENT 1 â†’ AGENT 2 PIPELINE TEST =====\n")

agent1_updates = load_agent1_output()
print("ğŸ” Agent 1 JSON Loaded:")
print(json.dumps(agent1_updates, indent=4))

test_profile = {
    "name": "Amina Test",
    "country": "Ghana",
    "dob": "2000-01-01",
    "program_start": "2025-01-05",
}

print("\nğŸ“˜ Using Test Profile:")
print(json.dumps(test_profile, indent=4))

output = generate_timeline_and_checklist(test_profile, agent1_updates)

print("\n\n===== AGENT 2 FINAL OUTPUT (TIMELINE + CHECKLIST) =====\n")
print(json.dumps(output, indent=4))

print("\n\n===== VALIDATION COMPLETE â€” PIPELINE WORKS =====\n")

# ## Section 10 â€” Launch Streamlit app with ngrok (optional)

# This cell:

# - Runs `streamlit run app.py`
# - Opens a public URL using ngrok

# Use this if you want to demo the app live.

!pip install pyngrok -q
!pip install streamlit -q

from pyngrok import ngrok
import time, getpass
import subprocess

# Kill old tunnels / servers
ngrok.kill()

# Start Streamlit in background
subprocess.Popen(
    ["streamlit", "run", "app.py", "--server.address", "0.0.0.0", "--server.port", "8501"],
    stdout=subprocess.DEVNULL,
    stderr=subprocess.DEVNULL,
)

time.sleep(5)

NGROK_TOKEN = getpass.getpass("Enter REAL ngrok token: 36OXQteErUiF8L10PVU3Jo3HTll_4brmcjGxBfs9YqwAtpNgP") ## copy this and paste
ngrok.set_auth_token(NGROK_TOKEN)

public_url = ngrok.connect("http://0.0.0.0:8501")
print("ğŸŒ Public URL:", public_url)

from agents_backend import chat_reply

test_profile = {
    "name": "Test",
    "country": "India",
    "dob": "2000-01-01",
    "program_start": "2024-08-15",
}

resp = chat_reply(
    test_profile,
    [{"role": "user", "content": "What are the H-1B requirements?"}],
)
print(resp[:800])

import agents_backend
print(agents_backend.__file__)

import importlib
import agents_backend

importlib.reload(agents_backend)   # force Python to read the new file

from agents_backend import chat_reply

test_profile = {
    "name": "Test",
    "country": "India",
    "dob": "2000-01-01",
    "program_start": "2024-08-15",
}

resp = chat_reply(
    test_profile,
    [{"role": "user", "content": "What are the H-1B requirements?"}],
)
print(resp[:800])

import importlib, agents_backend
importlib.reload(agents_backend)
from agents_backend import chat_reply

test_profile = {
    "name": "Test",
    "country": "India",
    "dob": "2000-01-01",
    "program_start": "2024-08-15",
}

resp = chat_reply(
    test_profile,
    [{"role": "user", "content": "What are the H-1B requirements?"}],
)
print(resp[:800])

!pkill -f "streamlit run" || echo "No previous Streamlit processes"
from pyngrok import ngrok
import time, getpass, subprocess

ngrok.kill()

proc = subprocess.Popen(
    ["streamlit", "run", "app.py", "--server.address", "0.0.0.0", "--server.port", "8501"],
    stdout=subprocess.DEVNULL,
    stderr=subprocess.DEVNULL,
)

time.sleep(5)

NGROK_TOKEN = getpass.getpass("Enter REAL ngrok token: ")
ngrok.set_auth_token(NGROK_TOKEN)
public_url = ngrok.connect("http://0.0.0.0:8501")
print("ğŸŒ Public URL:", public_url)